{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM55ghbo5QY7a4ugiu2e5lj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmarieta/disaster_tweet/blob/main/NLP_tweet_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 526,
      "metadata": {
        "id": "4qnTjn2QEhEt"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import xgboost as xgb\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate, LSTM, Embedding, Flatten, Dropout\n",
        "from keras.regularizers import l2\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import io\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import f1_score\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "W9AXZSiuSpjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"/content/train.csv\")\n",
        "test_df = pd.read_csv(\"/content/test.csv\")"
      ],
      "metadata": {
        "id": "YX-YKvraFSM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "id": "y6JIDgeu2VXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df[['id','keyword', 'location', 'text', 'target']]\n"
      ],
      "metadata": {
        "id": "IkHDHqf0z6cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[train_df[\"target\"] == '0'][\"text\"].values[1]"
      ],
      "metadata": {
        "id": "C1_Erhu8FkMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[train_df[\"target\"] == '1'][\"text\"].values[1]"
      ],
      "metadata": {
        "id": "oh8BPwLdFsDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
        "\n",
        "## let's get counts for the first 5 tweets in the data\n",
        "example_train_vectors = count_vectorizer.fit_transform(train_df[\"text\"][0:5])"
      ],
      "metadata": {
        "id": "X01iYYEOGCtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## we use .todense() here because these vectors are \"sparse\" (only non-zero elements are kept to save space)\n",
        "print(example_train_vectors[0].todense().shape)\n",
        "print(example_train_vectors[0].todense())"
      ],
      "metadata": {
        "id": "LIWJ5aO4GNHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_df[\"text\"]\n",
        "y = train_df[\"target\"]\n",
        "X_submission = test_df['text']\n",
        "X.fillna('', inplace=True)  # Replace np.nan with empty string\n",
        "X_submission.fillna('', inplace=True)"
      ],
      "metadata": {
        "id": "2_q6odtNKQj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jMc0Qgc1e4xG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_to_binary(value):\n",
        "    if value == '0':\n",
        "        return 0\n",
        "    elif value == '1':\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "9Mi1k2gSH4AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "y_train = y_train.map(lambda x: map_to_binary(x))\n",
        "y_test = y_test.map(lambda x: map_to_binary(x))"
      ],
      "metadata": {
        "id": "-JVIOnC8KbxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert binary vectors to TF-IDF representation\n",
        "tfidf_vectorizer = TfidfVectorizer(binary=True, max_features=5000)  # Adjust max_features as needed\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_submission_tfidf = tfidf_vectorizer.transform(X_submission)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "gPTB5q2UX7MY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Tokenize and pad sequences for training data\n",
        "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
        "max_sequence_length = max(len(seq) for seq in sequences_train)\n",
        "X_train_padded = pad_sequences(sequences_train, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_padded = pad_sequences(sequences_test, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "sequences_submission = tokenizer.texts_to_sequences(X_submission)\n",
        "X_submission_padded = pad_sequences(sequences_submission, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Load pre-trained GloVe word embeddings\n",
        "glove_path = '/content/glove.6B.50d.txt' # '/content/gdrive/MyDrive/glove.6B.50d.txt' # Replace with the path to your downloaded GloVe file\n",
        "embedding_dim = 50  # Should match the dimension of the GloVe file you downloaded\n",
        "\n",
        "embedding_index = {}\n",
        "with open(glove_path, 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_index[word] = coefs\n",
        "\n",
        "# Create an embedding matrix\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector\n",
        "\n",
        "# Create the model input\n",
        "X_word_embeddings = embedding_matrix[X_train_padded]\n",
        "X_test_embeddings = embedding_matrix[X_test_padded]\n",
        "X_submission_word_embeddings = embedding_matrix[X_submission_padded]"
      ],
      "metadata": {
        "id": "Dub3OjAhQxL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tfidf, X_val_tfidf, X_train_word_embeddings, X_val_word_embeddings, y_train, y_val = train_test_split(\n",
        "    X_train_tfidf, X_word_embeddings, y_train, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "-K1g_PjZV0wG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model input for word embeddings\n",
        "word_embeddings_input = Input(shape=(max_sequence_length, embedding_dim))  # Fix the input shape\n",
        "word_embeddings_output = LSTM(64)(word_embeddings_input)\n",
        "word_embeddings_model = Model(inputs=word_embeddings_input, outputs=word_embeddings_output)\n"
      ],
      "metadata": {
        "id": "63KWTBTYPz-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the word embeddings to match the number of features in TF-IDF\n",
        "X_train_word_embeddings_flat = X_train_word_embeddings.reshape((X_train_word_embeddings.shape[0], -1))\n",
        "X_val_word_embeddings_flat = X_val_word_embeddings.reshape((X_val_word_embeddings.shape[0], -1))\n",
        "X_test_word_embeddings_flat = X_test_embeddings.reshape((X_test_embeddings.shape[0], -1))\n",
        "X_submission_embeddings_flat = X_submission_word_embeddings.reshape((X_submission_word_embeddings.shape[0], -1))"
      ],
      "metadata": {
        "id": "cev8B7dCdf7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tfidf_dense = X_train_tfidf.toarray()\n",
        "X_test_tfidf_dense = X_test_tfidf.toarray()\n",
        "X_submission_tfidf_dense = X_submission_tfidf.toarray()\n",
        "# Concatenate the representations\n",
        "X_train_combined = np.concatenate((X_train_tfidf_dense, X_train_word_embeddings_flat), axis=1)\n",
        "X_test_combined = np.concatenate((X_test_tfidf_dense, X_test_word_embeddings_flat), axis=1)\n",
        "X_submission_combined = np.concatenate((X_submission_tfidf_dense, X_submission_embeddings_flat), axis=1)\n",
        "# Build a simple model\n",
        "model_input = Input(shape=(X_train_combined.shape[1],))\n",
        "dense_layer = Dense(256, activation='relu')(model_input)\n",
        "dropout_layer = Dropout(0)(dense_layer)\n",
        "dense_layer = Dense(256, activation='relu')(dropout_layer)\n",
        "dropout_layer = Dropout(0)(dense_layer)\n",
        "dense_layer = Dense(256, activation='relu')(dropout_layer)\n",
        "dropout_layer = Dropout(0)(dense_layer)\n",
        "output_layer = Dense(1, activation='sigmoid')(dropout_layer)\n",
        "model = Model(inputs=model_input, outputs=output_layer)"
      ],
      "metadata": {
        "id": "gsc7yA7TQN5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_decay(epoch):\n",
        "    initial_learning_rate = 0.02  # Adjust this based on your preference\n",
        "    decay = 0.002  # Adjust the decay rate based on your preference\n",
        "    new_learning_rate = initial_learning_rate - epoch * decay\n",
        "    return max(new_learning_rate, 0.0001)"
      ],
      "metadata": {
        "id": "WV3NCxl3sOEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "\n",
        "# Define the learning rate schedule\n",
        "initial_learning_rate = 0.01\n",
        "decay_steps = 10  # Adjust as needed\n",
        "decay_rate = 0.9  # Adjust as needed\n",
        "staircase = False  # If True, the learning rate will decay in discrete steps\n",
        "\n",
        "# Create the learning rate scheduler\n",
        "lr_scheduler = ExponentialDecay(\n",
        "    initial_learning_rate=initial_learning_rate,\n",
        "    decay_steps=decay_steps,\n",
        "    decay_rate=decay_rate,\n",
        "    staircase=staircase\n",
        ")\n",
        "\n",
        "optimizer = Adam(learning_rate=lr_scheduler)\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train_combined, y_train, epochs=20, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "IKCNd4YcQUc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "T4WF4tZLfocJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions on the training data\n",
        "y_pred_test = model.predict(X_test_combined)\n",
        "\n",
        "# Convert predictions to binary (0 or 1) based on a threshold (e.g., 0.5)\n",
        "threshold = 0.5\n",
        "y_pred_binary = (y_pred_test > threshold).astype(int)\n",
        "\n",
        "# Compute the F1 score\n",
        "f1 = f1_score(y_test, y_pred_binary)\n",
        "print(\"F1 Score:\", f1)"
      ],
      "metadata": {
        "id": "8oEq0wQbGs3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission = pd.read_csv(\"/content/sample_submission.csv\")"
      ],
      "metadata": {
        "id": "-FN1vt9OG9yC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_submission_combined[0].nonzero())\n"
      ],
      "metadata": {
        "id": "3GTJh1srUU-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_submission[0:10])"
      ],
      "metadata": {
        "id": "UvwLhf_bShi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions on the training data\n",
        "y_pred_submission = model.predict(X_submission_combined)\n",
        "\n",
        "# Convert predictions to binary (0 or 1) based on a threshold (e.g., 0.5)\n",
        "threshold = 0.5\n",
        "y_pred_binary = (y_pred_submission > threshold).astype(int)"
      ],
      "metadata": {
        "id": "VCMVH2eSiLUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred_binary[0:10])"
      ],
      "metadata": {
        "id": "zzSBoUrKpqaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission[\"target\"] = y_pred_binary"
      ],
      "metadata": {
        "id": "_HC5545cHCSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission.head()"
      ],
      "metadata": {
        "id": "skWP77eDHESd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission.to_csv(\"/content/submission.csv\", index=False)"
      ],
      "metadata": {
        "id": "6I5Q2cRNHGLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S-myot1JHLIA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}